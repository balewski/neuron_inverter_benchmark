comment: LR sched and BS optimized for 1 IPU traning
#  full scale training w/ 1 LSTM, no val
# needs toolbox/ModelLstm.py
# see fro results: https://docs.google.com/document/d/1lwqxgStsugrbjM5SDRISv6ei5ILCOlLHF5ecG4dgefs/edit?usp=sharing

# choose data path depending on facility
data_path: /home/balewskij/neuron-data/  # GraphCore, M2000, lr66

probe_type: excite_4prB8kHz  

max_epochs: 153 
batch_size: 64 
# validation is on for epoch%period<len
validation_period: [10, 3] # [ period, lenOn] (epochs), lenOn=0 is Off

const_local_batch: True # True: faster, LR changes w/ num IPUs	
#max_samples_per_epoch: 8000  # uncoment to skip data

gc_m2000:
    replica_steps_per_iter: 30
    precompile_graph: False
    graph_caching: True

num_data_workers: 4  
log_freq_per_epoch: 3 
  
train_conf:
   warmup_epochs: 8
   optimizer: [AdamW, 1e-3] # initLR 
   LRsched: { plateau_patience: 4, reduceFactor: 0.11  }
   #LRsched: {  decay_epochs: 20, gamma: 0.09 }

model:
    myId:  a2f791f3a_ontra4
    comment: very optimized ML model, for GPUs
    # note, input & output shapes are derived for the data, see dataLoader

    Xconv_block: # CNN params
        filter: [30, 90, 180]
        kernel: [ 4,  4,  4]
        pool:   [ 4,  4,  4]

    lstm_block:  # LSTM params
        hidden_size: 120
        num_layers:  1

    batch_norm_flat: True 

    fc_block: # FC params w/o last layer
        dims: [ 512, 512, 512, 256, 128 ]
        dropFrac: 0.05  
    
         
# not tested on GC
save_checkpoint: False  # only when loss improves
resume_checkpoint: False  # False: always start over 
# warning: for multi-gpu & resume --> val_loss explodes - no loop over GPUs
