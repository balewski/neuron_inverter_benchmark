comment: LR sched and BS optimized for 1 IPU traning
#  used for code acceleration by GC team 

# choose data path depending on facility
#data_path: /home/balewskij/neuron-data/  # GraphCore, M2000, lr66
data_path: /localdata/datasets/neuron-data/

probe_type: excite_4prB8kHz # excite2_4prB8kHz  #  BBP2

max_epochs: 184
batch_size: 192
# validation is on for epoch%period<len
validation_period: [10, 4] # [ period, lenOn] (epochs), lenOn=0 is Off

const_local_batch: True # True: faster, LR changes w/ num IPUs	
#max_samples_per_epoch: 8000  # uncoment to skip data

gc_m2000:
    replica_steps_per_iter: 1
    graph_caching: True
    pseudoValidation: True
    stagger_delay_sec:  0
    enableSyntheticData: False
    gradientAccumulation: 15
    num_io_tiles: 40
    use_all_reduce: False
    prefetch_depth: 1
    rebatch_size: 20

fp16_inputs: True
fp16_model: True

num_data_workers: 2

log_freq_per_epoch: 3
  
train_conf:
   warmup_epochs: 5
   optimizer: [AdamW, 1e-3] # initLR
   LRsched: { plateau_patience: 8, reduceFactor: 0.11 }
   #LRsched: {  decay_epochs: 20, gamma: 0.09 }

model:
    myId:  a2f791f3a_ontra4
    comment: very optimized ML model, for GPUs
    # note, input & output shapes are derived for the data, see dataLoader

    conv_block: # CNN params
        filter: [30, 90, 180]
        kernel: [ 4,  4,  4]
        pool:   [ 4,  4,  4]

    batch_norm_flat: True 

    fc_block: # FC params w/o last layer
        dims: [ 512, 512, 512, 256, 128 ]
        dropFrac: 0.05
    
         
# not tested on GC
save_checkpoint: False  # only when loss improves
resume_checkpoint: False  # False: always start over 
# warning: for multi-gpu & resume --> val_loss explodes - no loop over GPUs
